{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's import all the required libraries\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import selenium\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from selenium.webdriver.common.keys import Keys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1: Write a python program to scrape data for “Data Analyst” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, experience_required. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Analyst” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"0d0ee742-2053-4153-b29f-a2044efde0aa\")>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for job search\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar for keyword\n",
    "search_job.send_keys('Data Analyst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"3a0acd32-1773-43c1-8a84-b82754ed8354\")>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for job Location bar search\n",
    "search_loc=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on location search bar for keyword\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e1683179-b1e7-4361-9cbd-1fd105c7d90d\")>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding search button\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do click using xpath function\n",
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that window is opened by webdriver , So now we will extract\n",
    "\n",
    "Job_title\n",
    "company_names\n",
    "experinece_list\n",
    "Locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"6e5e9f78-fbc4-4438-9800-df7a5789014c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"f7bb1ca9-4d1b-4d12-9fed-6d0fd1065934\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"28df61b5-0a9a-43c7-88a0-b255f2321d8f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"028fdcdd-0495-405b-9a0b-bfb3910bd1f6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"02224894-2a6f-4419-8605-6e3e564d6c8d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"1f1bee94-5d6f-4b91-aac3-dd7994855d7c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"a6441618-c60d-4688-a105-1a9bb9a8c48c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"33047a93-0635-429c-a520-9047f90e26ea\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"36f400c9-0d46-40cd-8305-e9db9c95eebc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"ca6e3d8a-ce01-4ab2-9bfb-423b04d0d217\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"d2ba29c8-1b98-4ac5-8827-c5235b74f89c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"2c57432a-f25e-43db-b5a1-929c6c019d76\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"98a9eaf3-de95-4934-8a3c-5fed91f12ba0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"5dae9d49-f3c5-4bb1-ba59-787504f4d72a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"6a8abc2a-9954-492b-817a-e6a0f49e42e0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"a87fc32c-4da4-4196-be91-a862622f51dd\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"25032b6e-26a5-4104-b8e2-e9cfc57863cf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"ac6d8051-da6e-4e2c-b3ea-c2b86a70133f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"d5b31cb5-2dc9-49ea-b149-7eeba634f91b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"2257d22f-c317-4d7c-b13e-aeccd6b19dc8\")>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#putting xpath for job_titles\n",
    "titles_tags=driver.find_elements_by_xpath('//a[@class=\"title fw500 ellipsis\"]')\n",
    "titles_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract text of job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Senior Data Analyst - Data Lineage',\n",
       " 'Data Analyst - Informatica MDM',\n",
       " 'Data analysts',\n",
       " 'Excel VBA Jobs Bangalore | VBA data analyst Jobs',\n",
       " 'Data Analyst (2positions)//immediate Joiners//bangalore',\n",
       " 'Business/Data Analyst - SSE/LA',\n",
       " 'Business Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Data Analyst - Category Demand Management (Revenue & Growth)',\n",
       " 'data analyst',\n",
       " 'RC HIA - Data Analyst - A2',\n",
       " 'Data Analyst (Contractual)',\n",
       " 'Procurement Data Analyst',\n",
       " 'Data Analyst',\n",
       " 'Hiring Consultant Data Analyst Contractual',\n",
       " 'Data Analyst/ Data Consultant (Various Levels – Senior/ Mid/ Junior)',\n",
       " 'Data Scientist / Data Analyst -Business Analyst',\n",
       " 'Senior Data Analyst',\n",
       " 'Senior Data Analyst']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a loop to extract text of job_titles tags\n",
    "job_titles=[]\n",
    "for i in titles_tags:\n",
    "    job_titles.append(i.text)\n",
    "job_titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e77dd55f-542b-424a-aafa-153ac451a536\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"eb54f08f-4122-4349-a7fa-c1af6fd12996\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"795eefc0-5ce6-4907-8e40-61dd9764a9e4\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"7099d392-3385-4e85-9313-03225d4e5e68\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"aff6054f-1805-471b-8287-19e2d05a1518\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"a91c72ac-50c1-4572-b9ab-40e2b57cf5a1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"4c1cad06-7293-45dd-9189-1a20414bd42f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"1436ecbc-5bc4-4762-8684-5c0d7c0e0861\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"2d41adae-70d4-4d89-9559-346aa4fc0537\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"97966d1d-f994-463b-a477-189a349f0a34\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"4cf2534a-20ef-429b-a655-ff45cd06b08e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"f2c607a9-4aae-444a-b100-1ffa492d6ba7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"15c05797-d4ec-4802-99f6-fa661912b238\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"f72b208a-375e-4356-a863-07450dbb80c5\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"a39d9ad1-d016-4691-a6a6-b7fde8af50bf\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"46b0caa6-5d7d-48b3-afb3-2102f61ad15a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"adcd81ec-35e1-40bd-b54d-476901ab4ba3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e149460e-6827-45ed-94c3-e0500b99a046\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e6018480-0bc4-4cb5-8733-5a41fa855ddc\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"6ea73011-d1ef-4a61-b078-ef96eadb403c\")>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#putting xpath for company_name\n",
    "companies_tags=driver.find_elements_by_xpath('//a[@class=\"subTitle ellipsis fleft\"]')\n",
    "companies_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Opex Global Services',\n",
       " 'Shell India Markets Private Limited',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'Mind Circus Innovation',\n",
       " 'Tech Mahindra Ltd.',\n",
       " 'CGI Information Systems and Management Consultants',\n",
       " 'dotSolved India Pvt., Ltd.',\n",
       " 'Myntra Designs Pvt. Ltd.',\n",
       " 'Myntra Designs Pvt. Ltd.',\n",
       " 'Myntra Designs Pvt. Ltd.',\n",
       " 'Disha Consultants',\n",
       " 'Pricewaterhouse Coopers Private Limited',\n",
       " 'Flipkart Internet Private Limited',\n",
       " 'Schneider Electric',\n",
       " 'IBM India Pvt. Limited',\n",
       " 'Flipkart Internet Private Limited',\n",
       " 'DVSUM INDIA PRIVATE LIMITED',\n",
       " 'Inflexion Analytix Private Limited',\n",
       " 'Cimpress India Private Limited',\n",
       " 'Eze Castle Integration Services (Formerly Alphaserve Technologies)']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a loop to extract text of companies_names tag\n",
    "companies_name=[]\n",
    "for i in companies_tags:\n",
    "    companies_name.append(i.text)\n",
    "companies_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"764692ea-9fd5-4e86-9772-c7d1cc4fd641\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"c6f03de0-8125-4f0a-a13f-03ffb0f45255\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"afdcd01c-9476-4d2a-a68b-45f4f1cb9fd9\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"49875181-b52b-4c76-b437-76623a6b94a1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"886c494a-c408-4f5e-addf-a0b4e605a855\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"b9749e23-5f78-4b79-96e8-f5a804f756d1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"a865f7d7-1863-4c4f-8931-574de47bb72f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"9e26b575-794a-4ffc-8b7e-329e684af16e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"0ae308f7-7467-45f1-a3a6-eb0c98442ff6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"3b536a9e-540d-47e1-a995-39ec548ac6eb\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"004194dd-075f-4c23-93ac-28b9fa7563ed\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"65a00b07-ffee-4f49-bc7f-c12334dbbed7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"207aa198-b0cd-4cff-b35b-853a4cdd3f2c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"5063c02f-ba00-4717-988e-28e19d32df93\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"bb742d8e-2685-41f1-a5dc-a9ac6c0db4e8\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"dd9cfadc-cc6f-46c9-b2fa-2b9f2558059e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"6766da77-e227-4a1d-9549-05017c0208ef\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"0ba9c96e-e0ed-49e8-8c10-3168ce3596c7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"796c6838-ed36-45ef-9b9f-af7379869269\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"34f632fc-05e5-4b74-86e2-8f09f2e715d1\")>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will extract experience\n",
    "exp_list=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi experience\"]/span')\n",
    "exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5-10 Yrs',\n",
       " '6-9 Yrs',\n",
       " '3-5 Yrs',\n",
       " '0-1 Yrs',\n",
       " '4-8 Yrs',\n",
       " '2-5 Yrs',\n",
       " '9-13 Yrs',\n",
       " '3-5 Yrs',\n",
       " '3-8 Yrs',\n",
       " '1-4 Yrs',\n",
       " '2-4 Yrs',\n",
       " '4-6 Yrs',\n",
       " '1-4 Yrs',\n",
       " '1-3 Yrs',\n",
       " '5-10 Yrs',\n",
       " '2-6 Yrs',\n",
       " '5-10 Yrs',\n",
       " '0-3 Yrs',\n",
       " '3-8 Yrs',\n",
       " '6-11 Yrs']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a loop for exp_list\n",
    "experience=[]\n",
    "for i in exp_list:\n",
    "    experience.append(i.text)\n",
    "experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e4094df5-1b08-4127-aae0-029a35cf00f6\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"84982830-c0be-4b46-8412-71dec9e871ec\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e52f51ba-8b01-43e5-9b4b-f0f987ec28ba\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"999e3ad5-ee0e-4fc2-89f6-35ec3f3c10d3\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e15144ab-1d7c-43bf-82b3-4805b17142a1\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"187db26e-df80-4c76-8737-24cb966ef44c\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"bb1f0f36-2f45-4c9a-b87e-18b7a0ab7ab7\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"ead21d76-b2f6-4a46-8869-a6c66a746621\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"870ba851-23f4-4611-9128-a13dfc78ede0\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"8e5b3019-954b-43e0-917e-fa81e803417d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"78335a8b-b86f-4cb3-b6fd-761bc2a5d83f\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"99dfe69f-28f0-436a-8bc5-57e1ee3bde85\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"d61ca9d0-52a9-4db8-a0d1-ce1c781fba03\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"fd4be9fc-f281-4131-92ed-5718027e3cba\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"acc79400-52d4-4e9e-aa4f-667b4ccee43a\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"fb5ff656-1a9c-47e5-9a16-47208a88db50\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"3cd1eb31-c43f-4ced-9338-1ed2b016524d\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"638a941f-1c4d-4a8a-ab46-fd02ba70219e\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"e322c1be-dcb3-4831-891c-9463035b900b\")>,\n",
       " <selenium.webdriver.remote.webelement.WebElement (session=\"1cab5e375cf054ac2433993d39036ba0\", element=\"590a3c88-4edc-48a0-b52d-9ea526863000\")>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now we will extract job_location\n",
    "locations_list=driver.find_elements_by_xpath('//li[@class=\"fleft grey-text br2 placeHolderLi location\"]/span')\n",
    "locations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Mysore/Mysuru, Coimbatore, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Chennai, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Pune, Chennai, Jaipur, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru(Bellandur)',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/Gurugram, Chennai, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Hyderabad/Secunderabad, Bangalore/Bengaluru']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a loop for exp_list\n",
    "locations=[]\n",
    "for i in locations_list:\n",
    "    locations.append(i.text)\n",
    "locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20\n"
     ]
    }
   ],
   "source": [
    "print(len(job_titles),len(companies_name),len(experience),len(locations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see that length of all elementa are equal thats good. Now we can create a table for all element of job list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all tags\n",
    "Jobs_DA=pd.DataFrame({})\n",
    "Jobs_DA['Title']=job_titles\n",
    "Jobs_DA['Company Name']=companies_name\n",
    "Jobs_DA['Experience']=experience\n",
    "Jobs_DA['Location']=locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Data Analyst - Data Lineage</td>\n",
       "      <td>Opex Global Services</td>\n",
       "      <td>5-10 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analyst - Informatica MDM</td>\n",
       "      <td>Shell India Markets Private Limited</td>\n",
       "      <td>6-9 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data analysts</td>\n",
       "      <td>IBM India Pvt. Limited</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Excel VBA Jobs Bangalore | VBA data analyst Jobs</td>\n",
       "      <td>Mind Circus Innovation</td>\n",
       "      <td>0-1 Yrs</td>\n",
       "      <td>Mysore/Mysuru, Coimbatore, Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Data Analyst (2positions)//immediate Joiners//...</td>\n",
       "      <td>Tech Mahindra Ltd.</td>\n",
       "      <td>4-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Business/Data Analyst - SSE/LA</td>\n",
       "      <td>CGI Information Systems and Management Consult...</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Business Data Analyst</td>\n",
       "      <td>dotSolved India Pvt., Ltd.</td>\n",
       "      <td>9-13 Yrs</td>\n",
       "      <td>Chennai, Bangalore/Bengaluru, Mumbai (All Areas)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Data Analyst - Category Demand Management (Rev...</td>\n",
       "      <td>Myntra Designs Pvt. Ltd.</td>\n",
       "      <td>1-4 Yrs</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                 Senior Data Analyst - Data Lineage   \n",
       "1                     Data Analyst - Informatica MDM   \n",
       "2                                      Data analysts   \n",
       "3   Excel VBA Jobs Bangalore | VBA data analyst Jobs   \n",
       "4  Data Analyst (2positions)//immediate Joiners//...   \n",
       "5                     Business/Data Analyst - SSE/LA   \n",
       "6                              Business Data Analyst   \n",
       "7                                       Data Analyst   \n",
       "8                                       Data Analyst   \n",
       "9  Data Analyst - Category Demand Management (Rev...   \n",
       "\n",
       "                                        Company Name Experience  \\\n",
       "0                               Opex Global Services   5-10 Yrs   \n",
       "1                Shell India Markets Private Limited    6-9 Yrs   \n",
       "2                             IBM India Pvt. Limited    3-5 Yrs   \n",
       "3                             Mind Circus Innovation    0-1 Yrs   \n",
       "4                                 Tech Mahindra Ltd.    4-8 Yrs   \n",
       "5  CGI Information Systems and Management Consult...    2-5 Yrs   \n",
       "6                         dotSolved India Pvt., Ltd.   9-13 Yrs   \n",
       "7                           Myntra Designs Pvt. Ltd.    3-5 Yrs   \n",
       "8                           Myntra Designs Pvt. Ltd.    3-8 Yrs   \n",
       "9                           Myntra Designs Pvt. Ltd.    1-4 Yrs   \n",
       "\n",
       "                                           Location  \n",
       "0                               Bangalore/Bengaluru  \n",
       "1                               Bangalore/Bengaluru  \n",
       "2                               Bangalore/Bengaluru  \n",
       "3    Mysore/Mysuru, Coimbatore, Bangalore/Bengaluru  \n",
       "4                               Bangalore/Bengaluru  \n",
       "5                               Bangalore/Bengaluru  \n",
       "6  Chennai, Bangalore/Bengaluru, Mumbai (All Areas)  \n",
       "7                               Bangalore/Bengaluru  \n",
       "8                               Bangalore/Bengaluru  \n",
       "9                               Bangalore/Bengaluru  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jobs_DA[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2: Write a python program to scrape data for “Data Scientist” Job position in “Bangalore” location. You have to scrape the job-title, job-location, company_name, full job-description. You have to scrape first 10 jobs data.\n",
    "This task will be done in following steps:\n",
    "1. first get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field and enter “Bangalore” in “enter the location” field.\n",
    "3. Then click the search button.\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"75251241517dad23eacc492340839201\", element=\"02c7aeaf-81d8-47ba-a3c2-5753c8fde4e9\")>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for job search\n",
    "search_job=driver.find_element_by_id('qsb-keyword-sugg')\n",
    "search_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on search bar for keyword\n",
    "search_job.send_keys('Data Scientist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<selenium.webdriver.remote.webelement.WebElement (session=\"75251241517dad23eacc492340839201\", element=\"be97ff9f-2593-4430-a6c7-2f4af16dd30c\")>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding element for job Location bar search\n",
    "search_loc=driver.find_element_by_id(\"qsb-location-sugg\")\n",
    "search_loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write on location search bar for keyword\n",
    "search_loc.send_keys(\"Bangalore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_btn=driver.find_element_by_xpath(\"//button[@class='btn']\")\n",
    "search_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can see that window is opened by webdriver , So now we will extract\n",
    "\n",
    "Job_title\n",
    "Job Location\n",
    "company name\n",
    "Full job Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "url=\"https://www.naukri.com/data-scientist-jobs-in-bangalore?k=data%20scientist&l=bangalore\"\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check the job title for the first job\n",
    "urls=[]\n",
    "for i in driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\"):\n",
    "    urls.append(i.get_attribute('href'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define variables\n",
    "job_title=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "job_description=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in urls[:10]:\n",
    "    driver.get(i)\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Fething job_title\n",
    "    try:\n",
    "        job_t=driver.find_element_by_xpath(\"//h1[@class='jd-header-title']\")\n",
    "        job_title.append(job_t.text)\n",
    "    except:\n",
    "        job_title.append('-')\n",
    "      \n",
    "    #fething job_location  \n",
    "    try:\n",
    "        job_loc=driver.find_element_by_xpath(\"//span[@class='location ']\")\n",
    "        job_location.append(job_loc.text)\n",
    "    except:\n",
    "        job_location.append('-')\n",
    "        \n",
    "    #fething comapny_name\n",
    "    try:\n",
    "        company_n=driver.find_element_by_xpath(\"//a[@class='pad-rt-8']\")\n",
    "        company_name.append(company_n.text)\n",
    "    except:\n",
    "        company_name.append('-')\n",
    "    \n",
    "    #fething job description\n",
    "    try:\n",
    "        job_desc=driver.find_element_by_xpath(\"//section[@class='job-desc']\")\n",
    "        job_description.append(job_desc.text)\n",
    "    except:\n",
    "        job_description.append('-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 10 10 10\n"
     ]
    }
   ],
   "source": [
    "print(len(job_title),len(job_location),len(company_name),len(job_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '-',\n",
       " 'Senior Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Lead Data Scientist',\n",
       " 'Data analytics / Data scientist intern (work from Home)',\n",
       " '-',\n",
       " 'Deputy Manager - Datalabs (Data Scientist)',\n",
       " '-',\n",
       " '-']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '-',\n",
       " 'Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru',\n",
       " 'Bangalore/Bengaluru',\n",
       " 'Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai (All Areas)',\n",
       " 'Kolkata, Bangalore/Bengaluru, Delhi / NCR',\n",
       " '-',\n",
       " 'Bangalore/Bengaluru',\n",
       " '-',\n",
       " '-']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '-',\n",
       " 'Fractal Analytics',\n",
       " 'FICO',\n",
       " 'Fractal Analytics',\n",
       " 'TalkValley LLC',\n",
       " '-',\n",
       " 'HDFC LIFE INSURANCE COMPANY LIMITED',\n",
       " '-',\n",
       " '-']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "company_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['-',\n",
       " '-',\n",
       " 'Job description\\nServe as primary technical lead on all phases of the projects from providing solutioning, experimentation and deployment.\\nWilling to get your hands dirty with data analysis as well as be comfortable delegating tasks to junior members on the project team, thus providing technical guidance and oversight to the overall project.\\nHands-on solutioning within the chosen AI platform to partner with both customer and internal data solution architects.\\nMust Have\\n8 years of experience delivering results from advanced analytics projects; with at least 2 years experience leading a project or large workstream\\nExperience working on data analytics problems in commercial problems space such as pricing, supply chain, marketing or customer experience. Proven track record with building and deploying supervised classification, regression, deep learning and unsupervised clustering models and time series analysis.\\n5 years of Knowledge and or experience with data transformations using SQL, leveraging SAS or SAS Viya platform.\\nStatistical model building with 5 years of experience\\nBasic understanding of machine learning models development in python and leveraging AWS.\\nBachelors Degree in Engineering, Mathematics Analytics, Economics or Business\\nNice to Have\\n5 years of Knowledge and experience with SAS or SAS Viya platform\\nExperience working with remote teams spread across the globe\\nMachine Learning modelling experience\\nPython programming to build ML models\\nAWS or equivalent other cloud based services knowledge SAS EG\\nExperience working in a data analytics COE or in an analytics consulting company\\nRoleTechnical Architect\\nIndustry TypeManagement Consulting\\nFunctional AreaIT Software - DBA, Datawarehousing\\nEmployment TypeFull Time, Permanent\\nRole CategoryProgramming & Design\\nEducation\\nUG :B.Tech/B.E. in Production/Industrial\\nPG :Any Postgraduate in Any Specialization\\nKey Skills\\nSupply chainData analysisSASTime series analysisArtificial IntelligenceConsultingMachine learningData analyticsSQLPython',\n",
       " 'Job description\\nJob Summary\\nThe candidate should be a Cybersecurity Data Scientist with a background in machine learning for cybersecurity applications.\\nHeShe will contribute, provide subject matter expertise in the area of cybersecurity for critical infrastructure systems.\\n\\nResearch, develop, design and implement machine learning algorithms for cyber threat detection in operational technology environments, under limited direction.\\nIdentify data types to enable detection of cyber events.\\nTest and validate developed algorithms on real operational data.\\nIdentify, define, and scope complex data analytics problems in the cybersecurity domain.\\nDevelop cross-domain strategies for increased network security and resiliency of the entire network.\\nRoleTeam Lead/Technical Lead\\nIndustry TypeAnalytics / KPO / Research\\nFunctional AreaIT Software - Network Administration, Security\\nEmployment TypeFull Time, Permanent\\nRole CategoryProgramming & Design\\nEducation\\nUG :B.Tech/B.E. in Computers\\nPG :Post Graduation Not Required\\nKey Skills\\nbig data analyticsArtificial IntelligenceMachine learningAnti money launderingNetwork securitySubject matter expertiseOperationsdigital transformationFraud detectionFICO',\n",
       " 'Job description\\nPosition Description:\\nThe Artificial Intelligence and Machine Learning (AIML) group at Fractal Analytics is actively involved in helping Fortune 500 companies by enabling them to discover how they can leverage their data using advanced and sophisticated AI/ML algorithms for which we are looking for Data Scientists with the capability to work on independent statistical and machine learning research/ projects. If you are a problem solver with a curiosity for exploring new techniques and technologies in AIML space, then we would like to talk with you.\\nJob Responsibilities:\\nAbility to understand a problem statement and implement analytical solutions & techniques independently with independently/proactively/thought-leadership\\nWork with stakeholders throughout the organization to identify opportunities for leveraging company/client data to drive business solutions\\nFast learner: ability to learn and pick up a new language/tool/ platform quickly\\nConceptualize, design, and deliver high-quality solutions and insightful analysis\\nConduct research and prototyping innovations; data and requirements gathering; solution scoping and architecture; consulting clients and client facing teams on advanced statistical and machine learning problems\\nCollaborate and coordinate with different functional teams (engineering and product development) to implement models and monitor outcomes\\nAbility to deliver AIML based solutions around a host of domains and problems, with some of them being: Customer Segmentation & Targeting, Propensity Modeling, Churn Modeling, Lifetime Value Estimation, Forecasting, Recommender Systems, Modeling Response to Incentives, Marketing Mix Optimization, Price Optimization\\nExperience Required:\\nMinimum 10+ years of experience in Machine Learning domain\\nExpert level proficiency in at least one of R and Python\\nAbility to create efficient solutions to complex problems. Strong skills in data-structures and ML algorithms\\nExperience of working on end-to-end data science pipeline: problem scoping, data gathering, EDA, modelling, insights, visualizations, monitoring and maintenance\\nProblem-solving: Ability to break the problem into small parts and applying relevant techniques to drive required outcomes\\nIntermediate to advanced knowledge of machine learning, probability theory, statistics, and algorithms. You will be required to discuss and use various algorithms and approaches on a daily basis\\nWe use regression, Bayesian methods, tree-based learners, SVM, RF, XGBOOST, time series modeling, dimensionality reduction, SEM, GLM, GLMM, clustering, Deep learning etc. on a regular basis. If you know few of them you are good to go\\nGood to Have:\\nExperience in one of the upcoming technologies like deep learning, NLP, image processing, recommender systems\\nExperience of working in on one or more domains:\\nCPG: pricing and promotion analytics, marketing analytics, trade promotions, supply chain management\\nBFSI: cross-sell, up-sell, campaign analytics, treasury analytics, fraud detection\\nHealthcare: medical adherence, medical risk profiling, EHR data, fraud-waste-abuse\\nExperience in working with Linux computing environment and use of command line tools like sed/awk\\nGood grasp on databases including RDBMS, NoSQL, MongoDB etc.\\nRoleAnalytics Manager\\nIndustry TypeMiscellaneous\\nFunctional AreaAnalytics & Business Intelligence\\nEmployment TypeFull Time, Permanent\\nRole CategoryAnalytics & BI\\nEducation\\nUG :Any Graduate in Any Specialization\\nKey Skills\\nRNoSQLArtificial IntelligenceData StructuresGoSupply Chain ManagementMarketing AnalyticsMachine LearningDeep LearningPython',\n",
       " 'Job description\\nWe are a group of tenured professors from Tier-1 business schools in US universities. We teach machine learning and deep learning to master and PhD students. For our efforts with MentorStudents.org, we are looking to hire a team of interns who are interested in learning data science and analyze data under our supervision.\\n\\nHours commitment: at least 42.5 hrs/week\\n\\nDuration of the internship: 3 months\\n\\nRequired skills: Communication skills, Python, and OOP\\n\\nPerks (besides getting paid): Learn from professors who teach machine learning in the US\\n+ internship completion certificate\\n+ endorsement of your candidacy to US companies for a remote job via our placement business unit\\n+ networking (check out https://MentorStudents.org)\\n\\nNext step / TEST for Selection:\\nThis test will help you present your resume better and improve your employability. Hence, it is in your interest to work on this test. Create a tutorial in Google doc on how to create a timeline in Tableau Public. Use events from your resume as events for the timeline. You are encouraged to look at https://public.tableau.com/en-us/gallery for inspiration. Send your Google doc link at jobs [at] mentorstudents [dot] org. Make sure to make the link publicly accessible so that our team can evaluate your work.\\nRoleTrainee\\nIndustry TypeIT Services & Consulting\\nFunctional AreaIT Software - eCommerce, Internet Technologies\\nEmployment TypeFull Time, Freelance/Homebased\\nRole CategoryOther\\nEducation\\nUG :Graduation Not Required\\nKey Skills\\nCommunication SkillsPython\\nJupyter\\nSkills highlighted with ‘‘ are preferred keyskills',\n",
       " '-',\n",
       " 'Job description\\nRoles & Responsibilities\\nDevelop end to end solutions including understanding the business need, aggregate & explore data, build and validate predictive modeling using the Machine Language approaches.\\nDevelopment of API in Python and Node JS environment\\nWork with AI, Business Insights, and InfoSec team to develop the APIs for models.\\nTrack and validate the model effectiveness and feed the learnings to fine-tune the models.\\n\\nCandidate Requirement\\nCandidate must have a background in Statistics/Mathematics/Operations Resources/Economics or Similar Subjects with overall analytical experience of >4 years\\nMust have hands on experience in Model Development or Validation for 2 years\\nCandidate must have experience in the latest ensemble techniques like Gradient boosting, Deep Learning, Random Forest, Optimization etc\\nMust be proficient in Python and Node JS. Knowledge of SQL is an add-on\\n\\n\\n\\nHDFC Life does not charge applicants any recruitment fee or a deposit in return for job offers. HDFC Life does not authorize any third party to make employment offers, refer or place candidates, for a fee charged to the applicant.\\nRoleOther\\nIndustry TypeInsurance\\nFunctional AreaAnalytics & Business Intelligence\\nEmployment TypeFull Time, Permanent\\nRole CategoryOther\\nEducation\\nUG :Any Graduate in Any Specialization\\nKey Skills\\nPredictive ModelingArtificial IntelligenceNode.JsStatisticsDeep LearningRandom ForestSQLModel DevelopmentData ScienceEnsembleJavascriptdata scientistPython',\n",
       " '-',\n",
       " '-']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of all tags\n",
    "Jobs_DS=pd.DataFrame({})\n",
    "Jobs_DS['Title']=job_title\n",
    "Jobs_DS['Job Location']=job_location\n",
    "Jobs_DS['Company Name']=company_name\n",
    "Jobs_DS['Job Description']=job_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Job Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Job description\\nServe as primary technical le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>FICO</td>\n",
       "      <td>Job description\\nJob Summary\\nThe candidate sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lead Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>Job description\\nPosition Description:\\nThe Ar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Data analytics / Data scientist intern (work f...</td>\n",
       "      <td>Kolkata, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>TalkValley LLC</td>\n",
       "      <td>Job description\\nWe are a group of tenured pro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Deputy Manager - Datalabs (Data Scientist)</td>\n",
       "      <td>Bangalore/Bengaluru</td>\n",
       "      <td>HDFC LIFE INSURANCE COMPANY LIMITED</td>\n",
       "      <td>Job description\\nRoles &amp; Responsibilities\\nDev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title  \\\n",
       "0                                                  -   \n",
       "1                                                  -   \n",
       "2                              Senior Data Scientist   \n",
       "3                                Lead Data Scientist   \n",
       "4                                Lead Data Scientist   \n",
       "5  Data analytics / Data scientist intern (work f...   \n",
       "6                                                  -   \n",
       "7         Deputy Manager - Datalabs (Data Scientist)   \n",
       "8                                                  -   \n",
       "9                                                  -   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0                                                  -   \n",
       "1                                                  -   \n",
       "2      Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "3                                Bangalore/Bengaluru   \n",
       "4  Gurgaon/Gurugram, Bangalore/Bengaluru, Mumbai ...   \n",
       "5          Kolkata, Bangalore/Bengaluru, Delhi / NCR   \n",
       "6                                                  -   \n",
       "7                                Bangalore/Bengaluru   \n",
       "8                                                  -   \n",
       "9                                                  -   \n",
       "\n",
       "                          Company Name  \\\n",
       "0                                    -   \n",
       "1                                    -   \n",
       "2                    Fractal Analytics   \n",
       "3                                 FICO   \n",
       "4                    Fractal Analytics   \n",
       "5                       TalkValley LLC   \n",
       "6                                    -   \n",
       "7  HDFC LIFE INSURANCE COMPANY LIMITED   \n",
       "8                                    -   \n",
       "9                                    -   \n",
       "\n",
       "                                     Job Description  \n",
       "0                                                  -  \n",
       "1                                                  -  \n",
       "2  Job description\\nServe as primary technical le...  \n",
       "3  Job description\\nJob Summary\\nThe candidate sh...  \n",
       "4  Job description\\nPosition Description:\\nThe Ar...  \n",
       "5  Job description\\nWe are a group of tenured pro...  \n",
       "6                                                  -  \n",
       "7  Job description\\nRoles & Responsibilities\\nDev...  \n",
       "8                                                  -  \n",
       "9                                                  -  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jobs_DS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3: In this question you have to scrape data using the filters available on the webpage\n",
    "\n",
    "You have to use the location and salary filter.\n",
    "You have to scrape data for “Data Scientist” designation for first 10 job results.\n",
    "You have to scrape the job-title, job-location, company_name, experience_required.\n",
    "The location filter to be used is “Delhi/NCR”\n",
    "The salary filter to be used is “3-6” lakhs\n",
    "The task will be done as shown in the below steps:\n",
    "1. First get the webpage https://www.naukri.com/\n",
    "2. Enter “Data Scientist” in “Skill,Designations,Companies” field.\n",
    "3. Then click the search button.\n",
    "4. Then apply the location filter and salary filter by checking the respective boxes\n",
    "4. Then scrape the data for the first 10 jobs results you get.\n",
    "5. Finally create a dataframe of the scraped data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 20 20 20 20\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Job Title</th>\n",
       "      <th>Job Location</th>\n",
       "      <th>Company Name</th>\n",
       "      <th>Experience</th>\n",
       "      <th>Salary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data analytics / Data scientist intern (work f...</td>\n",
       "      <td>Kolkata, Bangalore/Bengaluru, Delhi / NCR</td>\n",
       "      <td>TalkValley LLC</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>2,00,000 - 3,00,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Chaayos is Looking For Data Scientist</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>Chaayos (Sunshine Teahouse Pvt. Ltd.)</td>\n",
       "      <td>0-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru</td>\n",
       "      <td>Fractal Analytics</td>\n",
       "      <td>3-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Scientist / Data Analyst -Business Analyst</td>\n",
       "      <td>Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...</td>\n",
       "      <td>Inflexion Analytix Private Limited</td>\n",
       "      <td>0-3 Yrs</td>\n",
       "      <td>3,50,000 - 4,50,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Junior Data Scientist</td>\n",
       "      <td>Noida(Sector-59 Noida)</td>\n",
       "      <td>R Systems International Ltd.</td>\n",
       "      <td>3-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>We are hiring- Data Scientist +Python- Noida</td>\n",
       "      <td>Noida, Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>RANDSTAD INDIA PVT LTD</td>\n",
       "      <td>4-7 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Gurgaon/Gurugram, Delhi / NCR</td>\n",
       "      <td>Milliman India Pvt Ltd</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>Noida</td>\n",
       "      <td>NEC CORPORATION INDIA PRIVATE LIMITED</td>\n",
       "      <td>3-8 Yrs</td>\n",
       "      <td>Not disclosed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Data Analyst/Scientist Big Data, Statistical T...</td>\n",
       "      <td>Gurgaon/Gurugram</td>\n",
       "      <td>The Search House (A Div of JSD Search House Pv...</td>\n",
       "      <td>2-7 Yrs</td>\n",
       "      <td>5,00,000 - 8,00,000 PA.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Business Analyst- Data Scientist</td>\n",
       "      <td>Noida, Gurgaon/Gurugram</td>\n",
       "      <td>Wipro</td>\n",
       "      <td>2-5 Yrs</td>\n",
       "      <td>3,50,000 - 6,50,000 PA.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Job Title  \\\n",
       "0  Data analytics / Data scientist intern (work f...   \n",
       "1              Chaayos is Looking For Data Scientist   \n",
       "2                                     Data Scientist   \n",
       "3    Data Scientist / Data Analyst -Business Analyst   \n",
       "4                              Junior Data Scientist   \n",
       "5       We are hiring- Data Scientist +Python- Noida   \n",
       "6                                     Data Scientist   \n",
       "7                                     Data Scientist   \n",
       "8  Data Analyst/Scientist Big Data, Statistical T...   \n",
       "9                   Business Analyst- Data Scientist   \n",
       "\n",
       "                                        Job Location  \\\n",
       "0          Kolkata, Bangalore/Bengaluru, Delhi / NCR   \n",
       "1                                          New Delhi   \n",
       "2      Mumbai, Gurgaon/Gurugram, Bangalore/Bengaluru   \n",
       "3  Mumbai, Hyderabad/Secunderabad, Pune, Gurgaon/...   \n",
       "4                             Noida(Sector-59 Noida)   \n",
       "5               Noida, Gurgaon/Gurugram, Delhi / NCR   \n",
       "6                      Gurgaon/Gurugram, Delhi / NCR   \n",
       "7                                              Noida   \n",
       "8                                   Gurgaon/Gurugram   \n",
       "9                            Noida, Gurgaon/Gurugram   \n",
       "\n",
       "                                        Company Name Experience  \\\n",
       "0                                     TalkValley LLC    0-5 Yrs   \n",
       "1              Chaayos (Sunshine Teahouse Pvt. Ltd.)    0-5 Yrs   \n",
       "2                                  Fractal Analytics    3-7 Yrs   \n",
       "3                 Inflexion Analytix Private Limited    0-3 Yrs   \n",
       "4                       R Systems International Ltd.    3-5 Yrs   \n",
       "5                             RANDSTAD INDIA PVT LTD    4-7 Yrs   \n",
       "6                             Milliman India Pvt Ltd    2-5 Yrs   \n",
       "7              NEC CORPORATION INDIA PRIVATE LIMITED    3-8 Yrs   \n",
       "8  The Search House (A Div of JSD Search House Pv...    2-7 Yrs   \n",
       "9                                              Wipro    2-5 Yrs   \n",
       "\n",
       "                    Salary  \n",
       "0  2,00,000 - 3,00,000 PA.  \n",
       "1            Not disclosed  \n",
       "2            Not disclosed  \n",
       "3  3,50,000 - 4,50,000 PA.  \n",
       "4            Not disclosed  \n",
       "5            Not disclosed  \n",
       "6            Not disclosed  \n",
       "7            Not disclosed  \n",
       "8  5,00,000 - 8,00,000 PA.  \n",
       "9  3,50,000 - 6,50,000 PA.  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets run the chrome driver and url for naukri.com\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "url=\"https://www.naukri.com/\"\n",
    "driver.get(url)\n",
    "\n",
    "#finding element for job search\n",
    "driver.find_element_by_xpath('//input[@class=\"sugInp\"]').send_keys('Data Scientist')\n",
    "driver.find_element_by_xpath(\"//button[@class='btn']\").click()\n",
    "time.sleep(7)\n",
    "\n",
    "#lets tick the checkbox of location\n",
    "job_chk_box=driver.find_element_by_xpath(\"//span[@title='Delhi / NCR']\").click()\n",
    "time.sleep(4)\n",
    "job_chk_box\n",
    "\n",
    "\n",
    "#lets tick the checkbox of salary range\n",
    "search_salary=driver.find_element_by_xpath(\"//span[@title='3-6 Lakhs']\").click()\n",
    "time.sleep(4)\n",
    "search_salary\n",
    "\n",
    "\n",
    "#lets create emptylist of title,location,company name,exp,salary and scrape the data\n",
    "\n",
    "job_titles=[]\n",
    "job_location=[]\n",
    "company_name=[]\n",
    "experience=[]\n",
    "salary_range=[]\n",
    "time.sleep(5)\n",
    "\n",
    "#lets scrape data for job_title\n",
    "job_title=driver.find_elements_by_xpath(\"//a[@class='title fw500 ellipsis']\")\n",
    "for i in job_title:\n",
    "    job_titles.append(i.text)\n",
    "    \n",
    "    \n",
    "#lets scrape job_location\n",
    "job_loc=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi location']\")\n",
    "for i in job_loc:   \n",
    "    job_location.append(i.text)\n",
    "    \n",
    "    \n",
    "#lets scrape data for company_name\n",
    "comp_name=driver.find_elements_by_xpath(\"//a[@class='subTitle ellipsis fleft']\")\n",
    "for i in comp_name:    \n",
    "    company_name.append(i.text)\n",
    "  \n",
    "    \n",
    "#lets scrape data for experience\n",
    "exp=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi experience']\")\n",
    "for i in exp:    \n",
    "    experience.append(i.text)\n",
    "   \n",
    "    \n",
    "#lets scrape data for Salary range\n",
    "salary=driver.find_elements_by_xpath(\"//li[@class='fleft grey-text br2 placeHolderLi salary']\")\n",
    "for i in salary:  \n",
    "    salary_range.append(i.text)\n",
    "    \n",
    "print(len(job_titles),len(job_location),len(company_name),len(experience),len(salary_range))\n",
    "\n",
    "#Lets Create Dataframe for our data                                                          \n",
    "DS_Jobs=pd.DataFrame({})                                                            \n",
    "DS_Jobs['Job Title']=job_titles\n",
    "DS_Jobs['Job Location']=job_location\n",
    "DS_Jobs['Company Name']=company_name\n",
    "DS_Jobs['Experience']=experience\n",
    "DS_Jobs['Salary']=salary_range\n",
    "DS_Jobs[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6 : Scrape data of first 100 sunglasses listings on flipkart.com. You have to scrape four attributes:\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. Discount %\n",
    "\n",
    "To scrape the data you have to go through following steps:\n",
    "1. Go to flipkart webpage by url https://www.flipkart.com/\n",
    "2. Enter “sunglasses” in the search field where “search for products, brands and more” is written and click the search icon\n",
    "3. after that you will reach to a webpage having a lot of sunglasses. From this page you can scrap the required data as usual.\n",
    "4. after scraping data from the first page, go to the “Next” Button at the bottom of the page , then click on it\n",
    "5. Now scrape data from this page as usual\n",
    "6. repeat this until you get data for 100 sunglasses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def flipkart_sunglasses(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome(r\"C:\\Webdrivers\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(10)\n",
    "    \n",
    "    #Let's navigate to search bar\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's search for sunglasses product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sunglasses')\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    sunglass_brand_name=[]\n",
    "    sunglass_description=[]\n",
    "    sunglass_price=[]\n",
    "    sunglass_discount=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<3:\n",
    "        time.sleep(3)\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "        for brand in brands:\n",
    "            sunglass_brand_name.append(brand.text)\n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "        for description in descriptions:\n",
    "            sunglass_description.append(description.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "        for price in prices:\n",
    "            sunglass_price.append(price.text)\n",
    "        \n",
    "        discounts=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "        for discount in discounts:\n",
    "            sunglass_discount.append(discount.text)\n",
    "        if j==0:\n",
    "            driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "        elif j==1:\n",
    "            driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]').click()\n",
    "        else:\n",
    "            pass\n",
    "        j+=1\n",
    "        \n",
    "    # Let's create a DataFrame for our data   \n",
    "    sunglasses_flip=pd.DataFrame({})\n",
    "    sunglasses_flip['Product Brand']=sunglass_brand_name[:100]\n",
    "    sunglasses_flip['Product Description']=sunglass_description[:100]\n",
    "    sunglasses_flip['Price of Product']=sunglass_price[:100]\n",
    "    sunglasses_flip['Discount on Product']=sunglass_discount[:100]\n",
    "    return sunglasses_flip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product Brand</th>\n",
       "      <th>Product Description</th>\n",
       "      <th>Price of Product</th>\n",
       "      <th>Discount on Product</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Aislin</td>\n",
       "      <td>UV Protection, Gradient Cat-eye Sunglasses (58)</td>\n",
       "      <td>₹485</td>\n",
       "      <td>68% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Round Sunglasses (Free Size)</td>\n",
       "      <td>₹209</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>Mirrored, UV Protection Wayfarer Sunglasses (F...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>50% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PIRASO</td>\n",
       "      <td>UV Protection Aviator Sunglasses (Free Size)</td>\n",
       "      <td>₹349</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fastrack</td>\n",
       "      <td>UV Protection Rectangular Sunglasses (Free Size)</td>\n",
       "      <td>₹666</td>\n",
       "      <td>16% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>GANSTA</td>\n",
       "      <td>UV Protection Aviator Sunglasses (57)</td>\n",
       "      <td>₹349</td>\n",
       "      <td>82% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Aislin</td>\n",
       "      <td>UV Protection, Gradient Butterfly, Over-sized ...</td>\n",
       "      <td>₹598</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Silver Kartz</td>\n",
       "      <td>UV Protection Aviator Sunglasses (88)</td>\n",
       "      <td>₹255</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Aislin</td>\n",
       "      <td>UV Protection, Gradient Wayfarer, Cat-eye Sung...</td>\n",
       "      <td>₹358</td>\n",
       "      <td>78% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Aislin</td>\n",
       "      <td>UV Protection Wayfarer Sunglasses (58)</td>\n",
       "      <td>₹475</td>\n",
       "      <td>77% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Product Brand                                Product Description  \\\n",
       "0         Aislin    UV Protection, Gradient Cat-eye Sunglasses (58)   \n",
       "1         PIRASO         UV Protection Round Sunglasses (Free Size)   \n",
       "2       Fastrack  Mirrored, UV Protection Wayfarer Sunglasses (F...   \n",
       "3         PIRASO       UV Protection Aviator Sunglasses (Free Size)   \n",
       "4       Fastrack   UV Protection Rectangular Sunglasses (Free Size)   \n",
       "..           ...                                                ...   \n",
       "95        GANSTA              UV Protection Aviator Sunglasses (57)   \n",
       "96        Aislin  UV Protection, Gradient Butterfly, Over-sized ...   \n",
       "97  Silver Kartz              UV Protection Aviator Sunglasses (88)   \n",
       "98        Aislin  UV Protection, Gradient Wayfarer, Cat-eye Sung...   \n",
       "99        Aislin             UV Protection Wayfarer Sunglasses (58)   \n",
       "\n",
       "   Price of Product Discount on Product  \n",
       "0              ₹485             68% off  \n",
       "1              ₹209             82% off  \n",
       "2              ₹499             50% off  \n",
       "3              ₹349             78% off  \n",
       "4              ₹666             16% off  \n",
       "..              ...                 ...  \n",
       "95             ₹349             82% off  \n",
       "96             ₹598             72% off  \n",
       "97             ₹255             78% off  \n",
       "98             ₹358             78% off  \n",
       "99             ₹475             77% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "flipkart_sunglasses('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7: Scrape 100 reviews data from flipkart.com for iphone11 phone.\n",
    "You have to go the link: https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/p/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGREPBFGI&marketplace.\n",
    "As shown in the above page you have to scrape the tick marked attributes.\n",
    "These are\n",
    "1. Rating\n",
    "2. Review_summary\n",
    "3. Full review\n",
    "\n",
    "You have to scrape this data for first 100 reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def iphone_100_review(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome(r\"C:\\Webdrivers\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    rating=[]\n",
    "    review_summary=[]\n",
    "    full_review=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<10:\n",
    "    \n",
    "        ratings=driver.find_elements_by_xpath('//div[@class=\"_3LWZlK _1BLPMq\"]')\n",
    "        for i in ratings:\n",
    "            rating.append(i.text)\n",
    "        \n",
    "        reviews=driver.find_elements_by_xpath('//p[@class=\"_2-N8zT\"]')\n",
    "        for i in reviews:\n",
    "            review_summary.append(i.text)\n",
    "        \n",
    "        full_reviews=driver.find_elements_by_xpath('//div[@class=\"t-ZTKy\"]')\n",
    "        for i in full_reviews:\n",
    "            full_review.append(i.text)\n",
    "        time.sleep(10)    \n",
    "        \n",
    "        driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "        time.sleep(10)\n",
    "        j+=1\n",
    "        time.sleep(10)\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    iphone_review=pd.DataFrame({})\n",
    "    iphone_review['Rating']=rating\n",
    "    iphone_review['Review Summary']=review_summary\n",
    "    iphone_review['Full Reviews']=full_review\n",
    "    return iphone_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rating</th>\n",
       "      <th>Review Summary</th>\n",
       "      <th>Full Reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>The Best Phone for the Money\\n\\nThe iPhone 11 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>Perfect product!</td>\n",
       "      <td>Amazing phone with great cameras and better ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>Worth every penny</td>\n",
       "      <td>Previously I was using one plus 3t it was a gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>Great product</td>\n",
       "      <td>Amazing Powerful and Durable Gadget.\\n\\nI’m am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Highly recommended</td>\n",
       "      <td>iphone 11 is a very good phone to buy only if ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>5</td>\n",
       "      <td>Brilliant</td>\n",
       "      <td>I have migrated from OP 7pro... and trust me, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>5</td>\n",
       "      <td>Wonderful</td>\n",
       "      <td>This is my first ever I phone. Before this I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>5</td>\n",
       "      <td>Must buy!</td>\n",
       "      <td>It’s an amazing product from apple and the cam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>5</td>\n",
       "      <td>Terrific purchase</td>\n",
       "      <td>Upgraded from iphone 6 to 11 best phone for ip...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>5</td>\n",
       "      <td>Mind-blowing purchase</td>\n",
       "      <td>This will help you more. See if you are planni...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Rating         Review Summary  \\\n",
       "0       5              Brilliant   \n",
       "1       5       Perfect product!   \n",
       "2       5      Worth every penny   \n",
       "3       5          Great product   \n",
       "4       5     Highly recommended   \n",
       "..    ...                    ...   \n",
       "95      5              Brilliant   \n",
       "96      5              Wonderful   \n",
       "97      5              Must buy!   \n",
       "98      5      Terrific purchase   \n",
       "99      5  Mind-blowing purchase   \n",
       "\n",
       "                                         Full Reviews  \n",
       "0   The Best Phone for the Money\\n\\nThe iPhone 11 ...  \n",
       "1   Amazing phone with great cameras and better ba...  \n",
       "2   Previously I was using one plus 3t it was a gr...  \n",
       "3   Amazing Powerful and Durable Gadget.\\n\\nI’m am...  \n",
       "4   iphone 11 is a very good phone to buy only if ...  \n",
       "..                                                ...  \n",
       "95  I have migrated from OP 7pro... and trust me, ...  \n",
       "96  This is my first ever I phone. Before this I w...  \n",
       "97  It’s an amazing product from apple and the cam...  \n",
       "98  Upgraded from iphone 6 to 11 best phone for ip...  \n",
       "99  This will help you more. See if you are planni...  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "iphone_100_review('https://www.flipkart.com/apple-iphone-11-black-64-gb-includes-earpods-power-adapter/product-reviews/itm0f37c2240b217?pid=MOBFKCTSVZAXUHGR&lid=LSTMOBFKCTSVZAXUHGR3QP11A&marketplace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8: Scrape data for first 100 sneakers you find when you visit flipkart.com and search for “sneakers” in the search field.\n",
    "You have to scrape 4 attributes of each sneaker :\n",
    "1. Brand\n",
    "2. Product Description\n",
    "3. Price\n",
    "4. discount %\n",
    "Also note that all the steps required during scraping should be done through code only and not manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def flip_sneakers(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome(r\"C:\\Webdrivers\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    #Let's navigate to search bar\n",
    "    driver.find_element_by_xpath('//button[@class=\"_2KpZ6l _2doB4z\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's search for sneaker product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@title=\"Search for products, brands and more\"]').send_keys('sneakers')\n",
    "    driver.find_element_by_xpath('//button[@type=\"submit\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    sneaker_brand_name=[]\n",
    "    sneaker_description=[]\n",
    "    sneaker_price=[]\n",
    "    sneaker_discount=[]\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<3:\n",
    "        time.sleep(3)\n",
    "        brands=driver.find_elements_by_xpath('//div[@class=\"_2WkVRV\"]')\n",
    "        for brand in brands:\n",
    "            sneaker_brand_name.append(brand.text)\n",
    "        \n",
    "        descriptions=driver.find_elements_by_xpath('//a[contains(@class,\"IRpwTa\")]')\n",
    "        for description in descriptions:\n",
    "            sneaker_description.append(description.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"_30jeq3\"]')\n",
    "        for price in prices:\n",
    "            sneaker_price.append(price.text)\n",
    "        \n",
    "        discounts=driver.find_elements_by_xpath('//div[@class=\"_3Ay6Sb\"]')\n",
    "        for discount in discounts:\n",
    "            sneaker_discount.append(discount.text)\n",
    "        if j==0:\n",
    "            driver.find_element_by_xpath('//a[@class=\"_1LKTO3\"]').click()\n",
    "        elif j==1:\n",
    "            driver.find_element_by_xpath('//nav[@class=\"yFHi8N\"]/a[12]').click()\n",
    "        else:\n",
    "            pass\n",
    "        j+=1\n",
    "        \n",
    "    # Let's create a DataFrame for our data   \n",
    "    sneakers_flipkart=pd.DataFrame({})\n",
    "    sneakers_flipkart['Brand Name']=sneaker_brand_name[:100]\n",
    "    sneakers_flipkart['Product Descriptions']=sneaker_description[:100]\n",
    "    sneakers_flipkart['Price']=sneaker_price[:100]\n",
    "    sneakers_flipkart['Discount %']=sneaker_discount[:100]\n",
    "    return sneakers_flipkart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Product Descriptions</th>\n",
       "      <th>Price</th>\n",
       "      <th>Discount %</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>JEETLAV</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹349</td>\n",
       "      <td>65% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BRUTON</td>\n",
       "      <td>Combo Pack Of 4 Casual Sneakers For Men</td>\n",
       "      <td>₹424</td>\n",
       "      <td>89% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Perfect &amp; Affordable Combo Pack of 02 Pairs Sn...</td>\n",
       "      <td>₹499</td>\n",
       "      <td>72% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Robbie jones</td>\n",
       "      <td>Casual Sneakers Shoes For Men Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Combo Pack of 4 Casual Sneakers With Sneakers ...</td>\n",
       "      <td>₹474</td>\n",
       "      <td>76% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Chevit</td>\n",
       "      <td>Speed Set of 5 Pairs Sneakers Outdoors Casuals...</td>\n",
       "      <td>₹759</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Ktiz</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹399</td>\n",
       "      <td>60% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>ROCKFIELD</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹396</td>\n",
       "      <td>36% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Jack Diamond</td>\n",
       "      <td>Sneakers For Men</td>\n",
       "      <td>₹635</td>\n",
       "      <td>62% off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Bucik</td>\n",
       "      <td>Black Casual Shoes Synthetic Leather for Men S...</td>\n",
       "      <td>₹749</td>\n",
       "      <td>57% off</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brand Name                               Product Descriptions Price  \\\n",
       "0        JEETLAV                                   Sneakers For Men  ₹349   \n",
       "1         BRUTON            Combo Pack Of 4 Casual Sneakers For Men  ₹424   \n",
       "2         Chevit  Perfect & Affordable Combo Pack of 02 Pairs Sn...  ₹499   \n",
       "3   Robbie jones     Casual Sneakers Shoes For Men Sneakers For Men  ₹399   \n",
       "4         Chevit  Combo Pack of 4 Casual Sneakers With Sneakers ...  ₹474   \n",
       "..           ...                                                ...   ...   \n",
       "95        Chevit  Speed Set of 5 Pairs Sneakers Outdoors Casuals...  ₹759   \n",
       "96          Ktiz                                   Sneakers For Men  ₹399   \n",
       "97     ROCKFIELD                                   Sneakers For Men  ₹396   \n",
       "98  Jack Diamond                                   Sneakers For Men  ₹635   \n",
       "99         Bucik  Black Casual Shoes Synthetic Leather for Men S...  ₹749   \n",
       "\n",
       "   Discount %  \n",
       "0     65% off  \n",
       "1     89% off  \n",
       "2     72% off  \n",
       "3     60% off  \n",
       "4     76% off  \n",
       "..        ...  \n",
       "95    60% off  \n",
       "96    60% off  \n",
       "97    36% off  \n",
       "98    62% off  \n",
       "99    57% off  \n",
       "\n",
       "[100 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "flip_sneakers('https://www.flipkart.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9: Go to the link -\n",
    "https://www.myntra.com/shoes\n",
    "Set Price filter to “Rs. 6649 to Rs. 13099” , Color filter to “Black”,\n",
    "And then scrape First 100 shoes data you get. The data should include “Brand” of the shoes , Short Shoe description, price of the shoe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def shoes_myntra(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome(r\"C:\\Webdrivers\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    brand_name=[]\n",
    "    description=[]\n",
    "    product_price=[]\n",
    "    \n",
    "    #Let's apply filter for price\n",
    "    driver.find_element_by_xpath('//ul[@class=\"price-list\"]/li[2]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    #Let's apply filter for colour\n",
    "    driver.find_element_by_xpath('//span[@data-colorhex=\"black\"]').click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Let's scrape the data\n",
    "    j=0\n",
    "    while j<2:\n",
    "        time.sleep(5)\n",
    "        \n",
    "        brands=driver.find_elements_by_xpath('//h3[@class=\"product-brand\"]')\n",
    "        for q in brands:\n",
    "            brand_name.append(q.text)\n",
    "        \n",
    "        descs=driver.find_elements_by_xpath('//h4[@class=\"product-product\"]')\n",
    "        for q in descs:\n",
    "            description.append(q.text)\n",
    "        \n",
    "        prices=driver.find_elements_by_xpath('//div[@class=\"product-price\"]')\n",
    "        for q in prices:\n",
    "            product_price.append(q.text)\n",
    "        \n",
    "        driver.find_element_by_xpath('//a[@rel=\"next\"]').click()    \n",
    "        time.sleep(5)\n",
    "        j+=1\n",
    "    \n",
    "    # Let's create a DataFrame for our data\n",
    "    myntra_shoes=pd.DataFrame({})\n",
    "    myntra_shoes['Brand of the Shoes']=brand_name[0:100]\n",
    "    myntra_shoes['Short Shoes Description']=description[0:100]\n",
    "    myntra_shoes['Price of the Shoes']=product_price[0:100]\n",
    "    return myntra_shoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand of the Shoes</th>\n",
       "      <th>Short Shoes Description</th>\n",
       "      <th>Price of the Shoes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ADIDAS Originals</td>\n",
       "      <td>Men Superstar Sneakers</td>\n",
       "      <td>Rs. 8799Rs. 10999(20% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men JORDAN DELTA Basketball</td>\n",
       "      <td>Rs. 12495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UNDER ARMOUR</td>\n",
       "      <td>Men Charged Impulse Knit</td>\n",
       "      <td>Rs. 8999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Puma</td>\n",
       "      <td>Men Running Shoes</td>\n",
       "      <td>Rs. 7499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nike</td>\n",
       "      <td>Men KD13 EP Basketball Shoes</td>\n",
       "      <td>Rs. 12995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>PUMA Motorsport</td>\n",
       "      <td>Unisex Woven-Design Sneakers</td>\n",
       "      <td>Rs. 7999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Saint G</td>\n",
       "      <td>Women Leather Heeled Boots</td>\n",
       "      <td>Rs. 9900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Hush Puppies</td>\n",
       "      <td>Men Solid Leather Formal Derbys</td>\n",
       "      <td>Rs. 9999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Men Leather Monk Shoes</td>\n",
       "      <td>Rs. 9793Rs. 13990(30% OFF)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Heel &amp; Buckle London</td>\n",
       "      <td>Women Leather Pumps</td>\n",
       "      <td>Rs. 7192Rs. 8990(20% OFF)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Brand of the Shoes          Short Shoes Description  \\\n",
       "0       ADIDAS Originals           Men Superstar Sneakers   \n",
       "1                   Nike      Men JORDAN DELTA Basketball   \n",
       "2           UNDER ARMOUR         Men Charged Impulse Knit   \n",
       "3                   Puma                Men Running Shoes   \n",
       "4                   Nike     Men KD13 EP Basketball Shoes   \n",
       "..                   ...                              ...   \n",
       "95       PUMA Motorsport     Unisex Woven-Design Sneakers   \n",
       "96               Saint G       Women Leather Heeled Boots   \n",
       "97          Hush Puppies  Men Solid Leather Formal Derbys   \n",
       "98  Heel & Buckle London           Men Leather Monk Shoes   \n",
       "99  Heel & Buckle London              Women Leather Pumps   \n",
       "\n",
       "            Price of the Shoes  \n",
       "0   Rs. 8799Rs. 10999(20% OFF)  \n",
       "1                    Rs. 12495  \n",
       "2                     Rs. 8999  \n",
       "3                     Rs. 7499  \n",
       "4                    Rs. 12995  \n",
       "..                         ...  \n",
       "95                    Rs. 7999  \n",
       "96                    Rs. 9900  \n",
       "97                    Rs. 9999  \n",
       "98  Rs. 9793Rs. 13990(30% OFF)  \n",
       "99   Rs. 7192Rs. 8990(20% OFF)  \n",
       "\n",
       "[100 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "shoes=shoes_myntra('https://www.myntra.com/shoes')\n",
    "shoes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10: Go to webpage https://www.amazon.in/\n",
    "Enter “Laptop” in the search field and then click the search icon.\n",
    "Then set CPU Type filter to “Intel Core i7” and “Intel Core i9”.\n",
    "After setting the filters scrape first 10 laptops data. You have to scrape 3 attributes for each laptop:\n",
    "1. title\n",
    "2. Ratings\n",
    "3. Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Let's call the Function Definition\n",
    "\n",
    "def laptop(url):\n",
    "    \n",
    "    # Let's load the drivers and URL\n",
    "    driver = webdriver.Chrome(r\"C:\\Webdrivers\\chromedriver.exe\")\n",
    "    driver.get(url)\n",
    "    \n",
    "    \n",
    "    # Let's create empty list for storing the data\n",
    "    item_title=[]\n",
    "    price=[]\n",
    "    rating=[]\n",
    "    \n",
    "    #Let's search for laptop product and then click search button\n",
    "    driver.find_element_by_xpath('//input[@id=\"twotabsearchtextbox\"]').send_keys('Laptop')\n",
    "    driver.find_element_by_xpath('//input[@type=\"submit\"]').click()\n",
    "        \n",
    "    #Let's apply filter for \"Intel Core i7\"\n",
    "    driver.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/12598163031\"]/span/a',).click()\n",
    "    time.sleep(5)\n",
    "    \n",
    "    #Let's apply filter for \"Intel Core i9\"  \n",
    "    driver.find_element_by_xpath('//*[@id=\"p_n_feature_thirteen_browse-bin/16757432031\"]/span/a').click()\n",
    "        \n",
    "    # Let's scrape the data\n",
    "    titles=driver.find_elements_by_xpath('//span[@class=\"a-size-medium a-color-base a-text-normal\"]')\n",
    "    for i in titles:\n",
    "        item_title.append(i.text)\n",
    "    prices=driver.find_elements_by_xpath('//span[@class=\"a-price-whole\"]')\n",
    "    for i in prices:\n",
    "        price.append(i.text)\n",
    "    ratings=driver.find_elements_by_xpath('//div[@class=\"a-row a-size-small\"]/span[1]')   \n",
    "    for i in ratings:\n",
    "        rating.append(i.get_attribute('aria-label'))\n",
    "        \n",
    "    # Let's create a DataFrame for our data\n",
    "    amazon_laptops=pd.DataFrame({})\n",
    "    amazon_laptops['Title']=item_title[:10]\n",
    "    amazon_laptops['Price']=price[:10]\n",
    "    amazon_laptops['Rating']=rating[:10]\n",
    "    return amazon_laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>Price</th>\n",
       "      <th>Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...</td>\n",
       "      <td>86,990</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MSI GL65 Leopard 10SDK-069IN Intel Core i7-107...</td>\n",
       "      <td>1,20,240</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dell Alienware m15(R3) 15.6-inch FHD Gaming La...</td>\n",
       "      <td>1,98,590</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...</td>\n",
       "      <td>2,77,390</td>\n",
       "      <td>3.3 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(Renewed) HP EliteBook 820 G4 Laptop (CORE I5 ...</td>\n",
       "      <td>40,790</td>\n",
       "      <td>4.6 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>HP 14 Thin &amp; Light 14-inch FHD Laptop (11th Ge...</td>\n",
       "      <td>76,500</td>\n",
       "      <td>4.1 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HP Pavilion Gaming 10th Gen Intel Core i7 Proc...</td>\n",
       "      <td>83,077</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Asus ROG Zephyrus S Ultra Slim Gaming Laptop, ...</td>\n",
       "      <td>49,999</td>\n",
       "      <td>1.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Mi Notebook Horizon Edition 14 Intel Core i5-1...</td>\n",
       "      <td>47,190</td>\n",
       "      <td>4.3 out of 5 stars</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>(Renewed) Lenovo Thinkpad Yoga S1 Laptop (CORE...</td>\n",
       "      <td>1,35,490</td>\n",
       "      <td>3.0 out of 5 stars</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Title     Price  \\\n",
       "0  Lenovo IdeaPad Flex 5 11th Gen Intel Core i7 1...    86,990   \n",
       "1  MSI GL65 Leopard 10SDK-069IN Intel Core i7-107...  1,20,240   \n",
       "2  Dell Alienware m15(R3) 15.6-inch FHD Gaming La...  1,98,590   \n",
       "3  ASUS ZenBook Pro Duo Intel Core i9-10980HK 10t...  2,77,390   \n",
       "4  (Renewed) HP EliteBook 820 G4 Laptop (CORE I5 ...    40,790   \n",
       "5  HP 14 Thin & Light 14-inch FHD Laptop (11th Ge...    76,500   \n",
       "6  HP Pavilion Gaming 10th Gen Intel Core i7 Proc...    83,077   \n",
       "7  Asus ROG Zephyrus S Ultra Slim Gaming Laptop, ...    49,999   \n",
       "8  Mi Notebook Horizon Edition 14 Intel Core i5-1...    47,190   \n",
       "9  (Renewed) Lenovo Thinkpad Yoga S1 Laptop (CORE...  1,35,490   \n",
       "\n",
       "               Rating  \n",
       "0  4.6 out of 5 stars  \n",
       "1  3.0 out of 5 stars  \n",
       "2  3.0 out of 5 stars  \n",
       "3  3.3 out of 5 stars  \n",
       "4  4.6 out of 5 stars  \n",
       "5  4.1 out of 5 stars  \n",
       "6  4.3 out of 5 stars  \n",
       "7  1.0 out of 5 stars  \n",
       "8  4.3 out of 5 stars  \n",
       "9  3.0 out of 5 stars  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's call the Calling Function\n",
    "\n",
    "laptop('https://www.amazon.in/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
